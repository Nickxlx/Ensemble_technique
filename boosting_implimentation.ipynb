{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e20edae8-dd0e-42b6-843e-64912076e7db",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression ?\n",
    "\n",
    "Answer--> As it's a boosting technique which focuse on sequential traning of weak models to create a strong model. the main idea behind this to correct the mistakes made by the previous model by decreating the lose function ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726f9a3d-0f0e-4303-b49b-fc7e4cb3499b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b031e42-7b7f-4256-87d7-859c284986ed",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96415bcc-2355-4b7a-9294-e9b36a31b4aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.2941\n",
      "R-squared: 77.5582%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Create a DataFrame from the dataset\n",
    "X = pd.DataFrame(data=housing.data, columns=housing.feature_names)\n",
    "y = pd.Series(housing.target)\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# let's insialize our model\n",
    "gb_reg = GradientBoostingRegressor()  # default values \n",
    "gb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = gb_reg.predict(X_test)\n",
    "\n",
    "#Evaluation \n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3638aea0-2d11-4ff6-9fae-6362b53749fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee1c8dda-c265-4b58-94a2-86272395eed8",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6483128-7977-4988-8fef-240a0fb3533c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=2, n_estimators=100;, score=0.756 total time=   2.4s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=2, n_estimators=100;, score=0.757 total time=   2.4s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=2, n_estimators=100;, score=0.751 total time=   2.4s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=2, n_estimators=200;, score=0.784 total time=   4.7s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=2, n_estimators=200;, score=0.786 total time=   4.7s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=2, n_estimators=200;, score=0.777 total time=   4.7s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=2, n_estimators=300;, score=0.797 total time=   7.0s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=2, n_estimators=300;, score=0.797 total time=   7.0s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=2, n_estimators=300;, score=0.787 total time=   7.0s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.790 total time=   3.5s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.787 total time=   3.5s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.784 total time=   3.5s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=3, n_estimators=200;, score=0.810 total time=   6.9s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=3, n_estimators=200;, score=0.806 total time=   6.9s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=3, n_estimators=200;, score=0.802 total time=   6.9s\n",
      "[CV 1/3] END learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.819 total time=  10.5s\n",
      "[CV 2/3] END learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.813 total time=  10.4s\n",
      "[CV 3/3] END learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.810 total time=  10.4s\n",
      "[CV 1/3] END learning_rate=0.2, max_depth=2, n_estimators=100;, score=0.781 total time=   2.4s\n",
      "[CV 2/3] END learning_rate=0.2, max_depth=2, n_estimators=100;, score=0.783 total time=   2.4s\n",
      "[CV 3/3] END learning_rate=0.2, max_depth=2, n_estimators=100;, score=0.781 total time=   2.4s\n",
      "[CV 1/3] END learning_rate=0.2, max_depth=2, n_estimators=200;, score=0.804 total time=   4.7s\n",
      "[CV 2/3] END learning_rate=0.2, max_depth=2, n_estimators=200;, score=0.802 total time=   4.7s\n",
      "[CV 3/3] END learning_rate=0.2, max_depth=2, n_estimators=200;, score=0.795 total time=   4.7s\n",
      "[CV 1/3] END learning_rate=0.2, max_depth=2, n_estimators=300;, score=0.812 total time=   7.1s\n",
      "[CV 2/3] END learning_rate=0.2, max_depth=2, n_estimators=300;, score=0.809 total time=   7.0s\n",
      "[CV 3/3] END learning_rate=0.2, max_depth=2, n_estimators=300;, score=0.801 total time=   7.0s\n",
      "[CV 1/3] END learning_rate=0.2, max_depth=3, n_estimators=100;, score=0.808 total time=   3.5s\n",
      "[CV 2/3] END learning_rate=0.2, max_depth=3, n_estimators=100;, score=0.804 total time=   3.5s\n",
      "[CV 3/3] END learning_rate=0.2, max_depth=3, n_estimators=100;, score=0.800 total time=   3.5s\n",
      "[CV 1/3] END learning_rate=0.2, max_depth=3, n_estimators=200;, score=0.823 total time=   7.0s\n",
      "[CV 2/3] END learning_rate=0.2, max_depth=3, n_estimators=200;, score=0.817 total time=   7.0s\n",
      "[CV 3/3] END learning_rate=0.2, max_depth=3, n_estimators=200;, score=0.814 total time=   7.0s\n",
      "[CV 1/3] END learning_rate=0.2, max_depth=3, n_estimators=300;, score=0.829 total time=  10.5s\n",
      "[CV 2/3] END learning_rate=0.2, max_depth=3, n_estimators=300;, score=0.822 total time=  10.5s\n",
      "[CV 3/3] END learning_rate=0.2, max_depth=3, n_estimators=300;, score=0.819 total time=  10.5s\n",
      "[CV 1/3] END learning_rate=0.3, max_depth=2, n_estimators=100;, score=0.793 total time=   2.4s\n",
      "[CV 2/3] END learning_rate=0.3, max_depth=2, n_estimators=100;, score=0.796 total time=   2.4s\n",
      "[CV 3/3] END learning_rate=0.3, max_depth=2, n_estimators=100;, score=0.787 total time=   2.4s\n",
      "[CV 1/3] END learning_rate=0.3, max_depth=2, n_estimators=200;, score=0.812 total time=   4.7s\n",
      "[CV 2/3] END learning_rate=0.3, max_depth=2, n_estimators=200;, score=0.809 total time=   4.7s\n",
      "[CV 3/3] END learning_rate=0.3, max_depth=2, n_estimators=200;, score=0.800 total time=   4.7s\n",
      "[CV 1/3] END learning_rate=0.3, max_depth=2, n_estimators=300;, score=0.818 total time=   7.1s\n",
      "[CV 2/3] END learning_rate=0.3, max_depth=2, n_estimators=300;, score=0.813 total time=   7.1s\n",
      "[CV 3/3] END learning_rate=0.3, max_depth=2, n_estimators=300;, score=0.804 total time=   7.1s\n",
      "[CV 1/3] END learning_rate=0.3, max_depth=3, n_estimators=100;, score=0.816 total time=   3.5s\n",
      "[CV 2/3] END learning_rate=0.3, max_depth=3, n_estimators=100;, score=0.810 total time=   3.5s\n",
      "[CV 3/3] END learning_rate=0.3, max_depth=3, n_estimators=100;, score=0.806 total time=   3.5s\n",
      "[CV 1/3] END learning_rate=0.3, max_depth=3, n_estimators=200;, score=0.825 total time=   6.9s\n",
      "[CV 2/3] END learning_rate=0.3, max_depth=3, n_estimators=200;, score=0.815 total time=   7.0s\n",
      "[CV 3/3] END learning_rate=0.3, max_depth=3, n_estimators=200;, score=0.816 total time=   6.9s\n",
      "[CV 1/3] END learning_rate=0.3, max_depth=3, n_estimators=300;, score=0.828 total time=  10.4s\n",
      "[CV 2/3] END learning_rate=0.3, max_depth=3, n_estimators=300;, score=0.819 total time=  10.5s\n",
      "[CV 3/3] END learning_rate=0.3, max_depth=3, n_estimators=300;, score=0.819 total time=  10.4s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=3, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.1, 0.2, 0.3], &#x27;max_depth&#x27;: [2, 3],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300]},\n",
       "             scoring=&#x27;r2&#x27;, verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=3, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.1, 0.2, 0.3], &#x27;max_depth&#x27;: [2, 3],\n",
       "                         &#x27;n_estimators&#x27;: [100, 200, 300]},\n",
       "             scoring=&#x27;r2&#x27;, verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=3, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={'learning_rate': [0.1, 0.2, 0.3], 'max_depth': [2, 3],\n",
       "                         'n_estimators': [100, 200, 300]},\n",
       "             scoring='r2', verbose=3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param = {'learning_rate':[0.1,0.2,0.3],\n",
    "          'n_estimators':[100,200,300],\n",
    "          'max_depth':[2,3]}\n",
    "\n",
    "gcv = GridSearchCV(gb_reg, param_grid=param, cv = 3, scoring='r2', verbose = 3)\n",
    "gcv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa2a1958-6e8e-48c3-b5c9-4c8c80fbe347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 300}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "295f4ca7-a3b1-4f04-966d-443dcac6ea44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.2331\n",
      "R-squared: 82.2121%\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test data with tunned model \n",
    "y_pred = gcv.predict(X_test)\n",
    "\n",
    "#Evaluation \n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36e3260-ef98-4538-ac2f-43788300d367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f58a969-b9ba-451d-a481-84729cc3868e",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "Answer--> In Gradient Boosting, \"weak learner\" refers to a machine learning model that is relatively simple and performs only slightly better than random guessing on a given classification or regression problem. Weak learners are often used as the base or building blocks in ensemble methods like Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eb6bfe-3a6d-49c5-9ea5-edb007be56d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bf85612-cdf3-4381-934a-5e34153dc09c",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "Answer-->The intuition behind Gradient Boosting is to iteratively refine and improve predictions to create a generaliesed model by minimizing the lose function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc15c9d-3de7-4037-b6f9-dacbfd2ca384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a37fcdf2-4237-45bd-950d-c39bf862199a",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "Answer--> Here's how the Gradient Boosting algorithm builds an ensemble of weak learners:\n",
    "\n",
    "1. **Initialization**: It starts with an initial prediction, often just the mean of the target values for regression or the log-odds for classification.\n",
    "\n",
    "2. **First Weak Learner**: It trains a weak learner (usually a decision tree with limited depth) to predict the errors (residuals) between the initial prediction and the actual target values.\n",
    "\n",
    "3. **Updating Predictions**: The predictions of the first weak learner are added to the initial prediction, adjusting the model's predictions. This reduces the errors, but some errors often remain.\n",
    "\n",
    "4. **Iterative Process**: The algorithm repeats steps 2 and 3 multiple times (a specified number of iterations or until a stopping criterion is met). In each iteration, a new weak learner is trained to predict the remaining errors (residuals).\n",
    "\n",
    "5. **Weighted Combination**: The predictions of all the weak learners are combined in a weighted manner to create the final ensemble prediction. Initially, all weights are equal, but they are updated in each iteration to give more importance to the learners that reduce the errors the most.\n",
    "\n",
    "6. **Optimization**: Gradient Boosting uses gradient descent optimization to adjust the weights and the parameters of the weak learners. It minimizes a loss function (e.g., Mean Squared Error for regression or log loss for classification) by moving in the direction that decreases the loss.\n",
    "\n",
    "7. **Strong Ensemble**: Over iterations, the ensemble becomes stronger and more accurate as each weak learner focuses on the errors made by the previous ones. The final ensemble model combines the predictive power of all the weak learners to make highly accurate predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884479a1-664f-45d0-9275-1d33718b95a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eba1d286-0294-4658-81a6-f08300a20f4c",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?\n",
    "\n",
    "Anawer--> Here are the key steps involved in constructing the mathematical intuition of Gradient Boosting:\n",
    "\n",
    "1. **Initialization**:\n",
    "   - Initialize the ensemble's prediction as the simplest model, often just the mean of the target values for regression or the log-odds for classification.\n",
    "\n",
    "2. **Residual Calculation**:\n",
    "   - Calculate the residuals (errors) by subtracting the ensemble's prediction from the actual target values.\n",
    "\n",
    "3. **Weak Learner Training**:\n",
    "   - Train a weak learner (typically a decision tree with limited depth) on the residuals. The goal is to create a model that predicts the errors made by the current ensemble.\n",
    "\n",
    "4. **Prediction Update**:\n",
    "   - Update the ensemble's prediction by adding the predictions of the newly trained weak learner. This adjusts the model's predictions and reduces some of the errors.\n",
    "\n",
    "5. **Weighted Combination**:\n",
    "   - Combine the predictions of all weak learners constructed so far in a weighted manner. Initially, all weights are equal, but they are updated in each iteration to give more importance to the learners that contribute the most to reducing the overall error.\n",
    "\n",
    "6. **Gradient Descent**:\n",
    "   - Use gradient descent optimization to adjust the weights and the parameters of the weak learners. This involves calculating the gradient of a loss function (e.g., Mean Squared Error for regression or log loss for classification) with respect to the ensemble's predictions.\n",
    "\n",
    "7. **Iterative Process**:\n",
    "   - Repeat steps 3 to 6 for a specified number of iterations or until a stopping criterion is met. In each iteration, a new weak learner is trained to predict the residuals of the current ensemble.\n",
    "\n",
    "8. **Strong Ensemble**:\n",
    "   - Over iterations, the ensemble becomes stronger and more accurate as each weak learner focuses on the errors made by the previous ones. The final ensemble model combines the predictive power of all the weak learners to make highly accurate predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6f476-d862-4fec-9af7-19b4ea5a84b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
