{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a236a7f-cd70-4d27-b57c-ad12bacb5c0d",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?\n",
    "\n",
    "Answer--> An ensemble technique in machine learning refers to the approach of combining multiple individual models to create a stronger, more robust, and more accurate predictive model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6036a4d-e795-4520-8603-dd5eb0f80f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fa1b51b-26bb-4423-8b45-49998690eaaf",
   "metadata": {},
   "source": [
    "Q2. Why are ensemble techniques used in machine learning?\n",
    "\n",
    "Answer--> Ensemble techniques are a powerful tool in machine learning, capable of boosting the performance of individual models and improving the overall quality and reliability of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff56e36-0b84-4361-82af-745ee837fbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b4f5f21-5dc6-4ada-aacd-9ea6a2ebc430",
   "metadata": {},
   "source": [
    "Q3. What is bagging?\n",
    "\n",
    "Answer--> Bagging, short for \"Bootstrap Aggregating,\" is an ensemble technique in machine learning that involves creating multiple instances of a base model and training each instance on different subsets of the training data. The main idea behind bagging is to reduce variance and improve the overall performance and generalization ability of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa418865-5cf5-43bf-bf33-1ef5ee1a1ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73e240aa-da72-4aa1-b7e2-75d2baf1fef3",
   "metadata": {},
   "source": [
    "Q4. What is boosting?\n",
    "\n",
    "Answer--> Boosting is an ensemble learning technique in machine learning that aims to improve the performance of weak learners (models that are only slightly better than random guessing) by combining them sequentially. The main idea behind boosting is to train a series of models, where each new model is trained to correct the errors of the previous ones. This process allows boosting algorithms to create a strong predictive model by iteratively focusing on the instances that were misclassified by previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19436fa7-8b1a-4487-a62f-d86655dd7b7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5576a8c-aca3-4985-8bb0-2f2ccd326eeb",
   "metadata": {},
   "source": [
    "Q5. What are the benefits of using ensemble techniques?\n",
    "\n",
    "Answer--> Ensemble techniques offer several benefits in machine learning:\n",
    "\n",
    "- Improved Performance: Ensemble methods combine the predictions of multiple models, leading to better overall predictive performance compared to individual models. This results in higher accuracy, precision, recall, and F1-score.\n",
    "\n",
    "- Reduction of Overfitting: Ensembles help reduce overfitting by combining models that may have complementary strengths and weaknesses. This leads to models that are more robust and generalize better to new, unseen data.\n",
    "\n",
    "- Enhanced Robustness: Ensembles are less sensitive to noise and outliers in the data. Errors made by individual models can be offset by the collective wisdom of the ensemble, resulting in more reliable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d5abe1-e890-4458-b6fd-1cf4e5f23482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dcf41124-ac4d-4aca-83dc-6bab7760c129",
   "metadata": {},
   "source": [
    "Q6. Are ensemble techniques always better than individual models?\n",
    "\n",
    "Answer--> Ensemble techniques like Random Forest or Gradient Boosting are not always better than individual models. Their effectiveness depends on factors like the data's quality and quantity, the choice of the base model, computational resources, interpretability needs, time sensitivity, diversity of base models, and the complexity of hyperparameter tuning. Sometimes, a well-tuned individual model may perform equally well or better for a specific task. The decision to use an ensemble should be based on careful consideration of these factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae86bfe-7039-48e5-84f6-19b8921c6ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca706e2d-dd4c-4411-848f-e8e3bbec1649",
   "metadata": {},
   "source": [
    "Q7. How is the confidence interval calculated using bootstrap?\n",
    "\n",
    "Answer--> To calculate a confidence interval using the bootstrap method:\n",
    "\n",
    "1. Repeatedly resample your data with replacement to create many \"bootstrap samples.\"\n",
    "2. Calculate the statistic of interest (e.g., mean) for each bootstrap sample.\n",
    "3. Create a distribution of these statistics.\n",
    "4. Use percentiles of this distribution to determine the confidence interval, e.g., the 2.5th and 97.5th percentiles for a 95% confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc10774c-b8d8-4f59-842c-236aed409638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1bfe591-bfd0-4ff5-b08d-db5bfd913859",
   "metadata": {},
   "source": [
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?\n",
    "\n",
    "Answer--> Here are the steps involved in the bootstrap method:\n",
    "\n",
    "1. **Data Collection**: Start with your original dataset containing 'n' observations.\n",
    "\n",
    "2. **Resampling with Replacement**: Randomly select 'n' observations from the dataset, allowing for duplicates (sampling with replacement). This forms a new bootstrap sample.\n",
    "\n",
    "3. **Model Estimation**: Train your machine learning model (estimator) on the bootstrap sample. This involves fitting the model to the resampled data to learn the underlying patterns.\n",
    "\n",
    "4. **Prediction or Inference**: Use the trained model to make predictions on the original dataset or a separate validation/test dataset.\n",
    "\n",
    "5. **Repeat Steps 2-4**: Repeat steps 2 to 4 a large number of times (e.g., hundreds or thousands of times). Each time, you're creating a new bootstrap sample, training the model, and making predictions.\n",
    "\n",
    "6. **Collect Statistics**: Collect and record the statistic of interest from each iteration. This could be the performance metric of your model, such as accuracy, mean squared error, or any other relevant measure.\n",
    "\n",
    "7. **Calculate Variability**: With the collected statistics, you have a distribution of the statistic of interest. You can calculate various summary statistics of this distribution, such as the mean, median, standard deviation, percentiles, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e9fa04-c469-4b5c-9290-3536e9ceded7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25b70bf7-1e0b-40d8-b72e-969e2a570bed",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "795f02da-4072-43ce-b0a2-0dfd83dd2431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval: 15.55402929408595\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original sample data\n",
    "original_sample_mean = 15  # Mean height of the sample\n",
    "original_sample_std = 2    # Standard deviation of the sample\n",
    "sample_size = 50           # Size of the sample\n",
    "\n",
    "# Number of bootstrap samples\n",
    "num_bootstrap_samples = 10000\n",
    "\n",
    "# Create bootstrap sample means\n",
    "bootstrap_sample_means = []\n",
    "for _ in range(num_bootstrap_samples):\n",
    "    bootstrap_sample = np.random.normal(original_sample_mean, original_sample_std, sample_size)\n",
    "    bootstrap_sample_mean = np.mean(bootstrap_sample)\n",
    "    bootstrap_sample_means.append(bootstrap_sample_mean)\n",
    "\n",
    "# Calculate confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_sample_means, [97.5])\n",
    "\n",
    "print(\"95% Confidence Interval:\", confidence_interval[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd93759-9212-45c8-96db-404f22b9e018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
